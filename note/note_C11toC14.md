# 十一、强化学习 Reinforcement Learning

## 双老虎机案例

考虑面前有两个老虎机，分别为红色和蓝色。其中蓝色老虎机每次拉动拉杆有$P=1$的概率获得一美元，红色老虎机每次拉动拉杆有$P=0.75$的概率获得二美元，有$P=0.25$的概率获得零美元。求给定总拉动次数为$100$的情况下，获得最多美元的策略。

![双老虎机](./data/双老虎机.png)

解决：可以通过MDP的方法进行解决，只需要提前计算而不需要真正进行老虎机的拉动。最终结论为，玩红色老虎机效用为$150$美元，蓝色老虎机效用为$100$美元。

对于该案例，假设红色老虎机获得二美元和零美元的概率是未知的，而我们仅仅能观察到拉动红色老虎机的获利，再次考虑获得美元最多的策略：

- 这是一个学习问题，而非计划问题
- **强化学习**的概念由此引出，我们需要通过采样观察得到信息，并利用这些信息来进行判断，尽管这些信息可能不准确。

相较于MDP，强化学习的转移模型$T(s,a,s')$和奖励模型$R(s,a,s')$是未知的，需要环境问题本身给出观察结果

## 根据模型的学习

核心思想：根据采样估计转移模型和奖励模型的值$\hat{T}(s,a,s'),\hat{R}(s,a,s')$

![根据模型的学习](./data/根据模型的学习.png)

假设我们要求上海科技大学学生的期望年龄，即
$$
E[A]=\sum_aP(a)\cdot a=0.35\times 20 +...
$$
我们通过采样$[a_1,a_2,...,a_N]$，估算$\hat{P}(a)=\frac{num(a)}{N}$，进而求得$E[A]\approx\sum_a\hat{P}(a)\cdot a$

然而，对于**和模型无关的学习**，$E[A]\approx \frac{1}{N}\sum_ia_i$

## 和模型无关的学习

### 被动强化学习

核心思想：给定**固定策略$\pi(s)$**，按照该策略行动的同时，使用直接效用估计学习**状态的效用**

#### 直接效用估计

目标：在给定策略$\pi$的情况下计算每个状态的效用

步骤：

1. 根据$\pi$行动
2. 每到达一个状态，记录当前**折扣回报总和**（包括之前状态）
3. 平均这些样本

优点：易于理解，不需要$T$和$R$，最终能估计正确的均值

缺点：状态的效用并非相互独立的，每个状态都要分别学习，收敛速度慢

#### 策略评估

我们发现直接效用估计和策略评估十分相近，而策略评估考虑到了状态效用的关联性。策略评估有如下贝尔曼方程：
$$
\begin{align}
&V^\pi_{0}(s)=0\\
&V^\pi_{k+1}(s)\leftarrow\sum_{s'}T(s,\pi(s),s')[R(s,\pi(s),s')+\gamma V^\pi_k(s')]
\end{align}
$$
尽管我们不知道$T$和$R$的值，我们可以通过采样来估算
$$
\begin{align}
&sample_1=R(s,\pi(s),s^{'}_1)+\gamma V^\pi_k(s^{'}_1)\\
&sample_2=R(s,\pi(s),s^{'}_2)+\gamma V^\pi_k(s^{'}_2)\\
&...\\
&sample_n=R(s,\pi(s),s^{'}_n)+\gamma V^\pi_k(s^{'}_n)\\
\end{align}
$$

$$
V^\pi_{k+1}(s)\leftarrow \frac{1}{n}\sum_isample_i
$$

缺点：样本需要顺序采集

#### 时序差分学习

核心想法：在每次观察后立刻学习，更新$V(s)$

- $V(s)$的采样：$sample=R(s,\pi(s),s')+\gamma V^\pi(s')$
- $V(s)$的更新：$V^\pi(s)\leftarrow (1-\alpha)V^\pi(s)+(\alpha) sample$
- 同样的更新：$V^\pi(s)\leftarrow V^\pi(s)+\alpha(sample-V^\pi(s))$

其中蕴含着**指数平均移动**的思想：

- 运行中的插值更新：$\bar{x}_n=(1-\alpha)\cdot \bar{x}_{n-1}+\alpha \cdot x_n$
  $$
  \bar{x}_n=\frac{x_n+(1-\alpha)\cdot x_{n-1}+(1-\alpha)^2\cdot x_{n-2}+...}{1+(1-\alpha)+(1-\alpha)^2+...}
  $$

- 使最近的采样更加重要
- 降低的学习率$\alpha$能够使给出收敛的平均值

时序差分学习的限制性：

- 如果想将价值转变为新的策略，即$\pi(s)=argmax_aQ(s,a)=argmax_a\sum_{s'}T(s,a,s')[R(s,a,s')+\gamma V(s')]$，$T(s,a,s')$和$R(s,a,s')$是未知的

### 主动强化学习

目标：**学习最优的策略或效用**

#### Q学习 Q-Learning

- Q学习的价值迭代
  $$
  Q_{k+1}(s,a)\leftarrow \sum_{s'}T(s,a,s')[R(s,a,s')+\gamma max_{a'}Q_k(s',a')]
  $$

- 步骤：

  - 接收一个采样$(s,a,s',r)$
  - 考虑旧的估计：$Q(s,a)$
  - 考虑新的样本估计：$sample=R(s,a,s')+\gamma max_{a'}Q(s',a')$
  - 将新的估计并入运行平均值：$Q(s,a)\leftarrow (1-\alpha)Q(s,a)+(\alpha)[sample]$

特性：

- Q学习是一种与**策略无关的学习**，这意味着尽管选择不是最优的，Q学习也会收敛到最优策略

## 根据MDP和问题特性选择方法

![根据MDP和问题选择方法](./data/根据MDP和问题选择方法.png)

## 探索策略选择

### $\varepsilon$贪心

制定一个概率$P=\varepsilon$：

- 有$\varepsilon$的概率随机选择方向
- 有$1-\varepsilon$的概率选择当前最优策略

同时随着时间降低$\varepsilon$的值

### 探索方程

探索那些还未被充分探索的地方，最终将停止探索

**核心思想**：通过修改Q值来选择行动

效用方程定义为：
$$
f(u,n)=u+\frac{k}{n}
$$
其中$u$为Q值的估计，$k$为一个常数，$n$为该状态访问的次数

**Q值更新**：

- 通常情况下：$Q(s,a)\leftarrow_\alpha R(s,a,s')+\gamma max_{a'}Q(s',a')$
- 修改情况下：$Q(s,a)\leftarrow_\alpha R(s,a,s')+\gamma max_{a'}f(Q(s',a'),N(s',a'))$

### 近似Q学习

标准的Q学习需要维护记录所有状态的Q值表，而通常情况状态数量过多，难以遍历或记录

想法：通过经验学习少量的训练状态，将这些经验归纳到新的、相近的状态

#### 基于特征的表述

通过一数组的特征来描述一个状态，比如对于吃豆人，可能有以下特征：

- 距离最近鬼的距离
- 距离最近豆子的距离
- 鬼的数量
- 豆子的数量
- 吃豆人是否在一个通道内被鬼包夹？

通过以上的特征描述，我们可以用权重来得到任何状态的q函数：
$$
V(s)=w_1f_1(s)+w_2f_2(s)+...+w_nf_n(s)\\
Q(s,a)=w_1f_1(s,a)+w_2f_2(s,a)+...+w_nf_n(s,a)
$$
优势：所有的经验被总结为几个强有力的数字

劣势：各个状态可能有共同的特点，但实际上在价值上有很大的不同

步骤：

- 接收一个采样$(s,a,s',r)$
- 计算**差**：$difference=[r+\gamma max_{a'}Q(s',a')]-Q(s,a)$
- 近似特征值更新：$w_i\leftarrow w_i+\alpha[difference]f_i(s,a)$

Note：通常Q值更新，$Q(s,a)\leftarrow Q(s,a)+\alpha[difference]$

关于特征值更新的解释：调整有效特征的权重，如果发生错误，对引起该错误的特征降权重

##### 相关推广

- 多项式近似Q值：$Q(s,a)=w_{11}f_1(s,a)+w_{12}f_1(s,a)^2+w_{13}f_1(s,a)^3+...$

- 神经网络：$Q(s,a)=w_1f_1(s,a)+w_2f_2(s,a)+...+w_nf_n(s,a)$，其中参数也是经由训练得到
  $$
  w_m\leftarrow w_m+\alpha[r+\gamma max_a Q(s',a')-Q(s,a)]\frac{dQ}{dw_m}(s,a)
  $$

### 策略搜索

由于基于特征的表述预测性过于强烈，可能为了使特征最大，选择当前非奖励最大的行动

想法：学习使奖励最大化的政策，而不是预测奖励的值，从一个确定的解决方案开始（例如，近似Q学习），然后微调特征权重以找到更好的政策

最简单的策略搜索步骤：从一个初始的线性Q近似开始，改变每个特征的权重，测试该策略是否更好

# 十二、监督学习

监督学习是一个被定义的过程

- 目标：学习一个未知的目标函数$f$
- 输入：含有有**标签**$(x_j,y_j),y_j=f(x_j)$的训练集
- 输出：接近$f$的假设$h$

典型问题：

- 分类问题：学习具有离散的输出值$f$
- 回归问题：学习具有实数域的输出值的$f$
- 结构化预测：学习具有结构化输出的$f$

## 分类问题

分类问题定义为给定输入$x$，预测标签（分类）$y$

### 应用举例

#### 垃圾邮件过滤器 Spam Filter

- 输入：一封电子邮件
- 输出：垃圾邮件（spam）或正常邮件（ham）

初始化：获取大量已分类的样本邮件，每封都已标记垃圾邮件或正常邮件，期望预测新的、未来的电子邮件的标签

垃圾邮件的可能特征：

- 文字：FREE!
- 文本模式：$, CAPS
- 非文本内容

#### 数字识别 Digit Recognition

- 输入：图像，像素网格
- 输出：数字0~9

初始化：获得大量的示例图像，每张图像都标有一个数字，期望预测新的、未来的数字图像的标签

可能特征：

- 某一个像素是黑色的
- 形状特征：组成部分，曲率

### 基础贝叶斯分类器

该分类器是一种基于模型的方法，建立一种标签和特征都是随机变量的贝叶斯网络，实例化任何观察到的特征，并根据特征查询标签的分布情况

![naive bayes model](./data/naive bayes model.png)
$$
P(Y,F_1,...,F_n)=P(Y)\prod_iP(F_i|Y)
$$
其中$Y$为标签，$F_1,...,F_n$为特征，并且特征之间关于标签相互独立

#### 应用举例

##### 数字识别

特征为图像网格对应位置是否是开或关（二进制表示），开关定义为该像素点强度是否大于或小于$0.5$

![数字识别](./data/数字识别.png)

##### 邮件过滤器

特征为$W_i$，即第$i$位置的单词。由于每个位置都是独立的，可以认为所以位置都共享条件概率$P(W|Y)$，因此模型对词序或重新排序不敏感，且这些单词集合被称为“词包”

![邮件过滤器](./data/邮件过滤器.png)

#### 推断

目标：计算关于标签$Y$后验分布$P(Y|f_1,...,f_n)$

步骤：

1. 得到关于标签和特征的联合分布$P(Y,f_1,...,f_n)$
2. 对求得的联合分布归一化

### 测试数据集学习

对于基础贝叶斯网络，我们知道模型的结构，需要去估计概率表格（CPTs），而建立CPTs的过程需要训练数据

**经验增率**：对于每个结果$x$，$P_{ML}(x)=\frac{count(x)}{total\, samples}$。例如，我们从垃圾邮件中看到了1000个单词，其中有50是 "money"，因此设定$P(money|spam)=0.05$

Note: 该估计方法为最大似然估计（似然：给定参数数据的条件概率）

#### 最大似然估计MLE

MLE：最大似然估计是找出参数使概率最大

例如硬币问题，假设$P(heads)=\theta$，一个有着$\alpha_H$个正面和$\alpha_T$个反面的序列$D$的概率为
$$
P(D|\theta)=\theta^{\alpha_H}(1-\theta)^{\alpha_T}
$$
通过$ln$函数，可以等价为求得
$$
\begin{align}
\hat{\theta}&=arg\max_\theta\,\ln{P(D|\theta)}\\
&=arg\max_\theta\,\ln{\theta^{\alpha_H}(1-\theta)^{\alpha_T}}
\end{align}
$$
利用一阶导为零的性质，求得连续函数极值
$$
\begin{align}
\frac{d}{d\theta}\ln{P(D|\theta)}&=\frac{d}{d\theta}\ln{\theta^{\alpha_H}(1-\theta)^{\alpha_T}}\\
&=\frac{\alpha_H}{\theta}-\frac{\alpha_T}{1-\theta}=0
\end{align}
$$
最终解得
$$
\hat{\theta}_{MLE}=\frac{\alpha_H}{\alpha_H+\alpha_T}
$$

### 训练和测试

#### 重要概念

- 数据：被贴上标签的实例
  - 测试数据集
  - 保留数据集
  - 测试数据集
- 实验周期
  - 通过测试数据集学习参数
  - 在保留数据集上调整参数
  - 计算测试数据集的准确性
- 典型问题
  - 欠拟合（Underfitting）：测试集中训练效果很差
  - 过拟合（Overfitting）：测试集中训练效果很好，但测试集中效果很差

测试基线问题：需要定义一个标准，用来评估测试效果的好坏

测试基线过低的影响：给所有测试实例贴上训练集最常见的标签，如给所有过滤邮件都鉴定为正常邮件；如果测试数据集有偏差，精确度有可能会很高，如数据集中有66%的正常邮件，最终结果70%的准确率就不算好。

Note: 对于真实研究，基线通常是根据之前研究设定的

#### 泛化和过拟合

![过拟合例子](./data/过拟合例子.png)

过拟合案例：如上图所述，由于右下角的像素在训练集中未出现，因此识别该数字为3的概率为零

使用经验增率会过拟合数据集，因为尽管在训练中未见到相关实例，不意味在测试集中见不到，因此不能给未见过的时间赋值为零

过拟合发生原因：

- 训练样本过少或噪声过大，导致无法得出真实数据分布
- 与分类问题无关属性过多
- 模型表达能力过强，有很强的训练集记忆能力

#### 过拟合的平滑

##### 拉普拉斯平滑 Laplace Smoothing

想法：假设看到的每个结果都比实际情况多一次
$$
\begin{align}
P_{LAP}(x)&=\frac{c(x)+1}{\sum_x{c(x)+1}}\\
&=\frac{c(x)+1}{N+|X|}
\end{align}
$$

##### 拉普拉斯估计扩展

想法：假设看到的每个结果都比实际情况多$k$次
$$
P_{LAP,k}(x)=\frac{c(x)+k}{N+k|X|}
$$

##### 条件拉普拉斯估计

前提：需要每个$x$条件独立
$$
P_{LAP,k}(x|y)=\frac{c(x,y)+k}{c(y)+k|X|}
$$


Note: 经验增率是最大似然估计；拉普拉斯估计是最大可能参数估计

##### 线性插值 Linear Interpolation

当$|X|,|Y|$分别较大的情况时，拉普拉斯对$P(X|Y)$估计较差

线性插值想法：从数据中得到经验率$P(X)$，保证估计$P(X|Y)$和$P(X)$相差不大
$$
P_{LIN}(x|y)=\alpha \hat{P}(x|y)+(1-\alpha)\hat{P}(x)
$$

### 保留数据集超参数调整

通过在测试集上的学习，我们得出以下两种参数：

- 普通参数：$P(X|Y),P(Y)$
- 超参数：$k$（拉普拉斯估计），$\alpha$（线性插值）

普通参数是根据训练集得出的，而超参数需要在保留数据集上不断调试，直到找到表现最好的超参数

### 线性分类器

- 输入：特征值
- 权重：每个特征值有一个权重
- **激活**：加权特征和
- 输出：根据激活和问题类型决定，如二元分类器，当激活值为负数输出$-1$，为正数输出$1$

$$
activation_w(x)=\sum_i w_i\cdot f_i(x)=w\cdot f(x)
$$

有时在激活的计算会假如一个偏差（bias）
$$
\sum_i w_i\cdot f_i(x)+b=w\cdot f(x)+b
$$
Note: 偏差等价于加入一个常量特征

特性：

- 分离性：如果某些参数得到的训练集完全正确，则分离性为真
- 收敛性：如果训练集是可分离的，感知机最终会收敛

#### 举例：二元感知机

```pseudocode
初始化：weights = 0
循环：For each training instance:
	通过当前权重分类，计算activation:
		y = 1 if w*f(x) >= 0
		y = -1 if w*f(x) < 0
	如果分类正确:
		continue
	如果分类不正确:
		通过加减特征向量来调整权重向量:
			w = w + correcty * f(x)
```

#### 举例：多元感知机

- 每个分类的权重向量是：$w_y$
- 关于分类$y$的分数（激活）是：$w_y\cdot f(x)$
- 最高分数的分类获胜：$y=arg\max_yw_y\cdot f(x)$

```pseudocode
初始化：weights = 0
循环：For each training instance:
	分类：y = argmax y(wy * f(x))
	如果分类正确:
		continue
	如果分类不正确:
		降低错误答案的分数，提高正确答案的分数:
			wy = wy - f(x)
			wy(correct) = wy(correct) + f(x)
```

### 改进感知机（逻辑回归）

考虑之前的感知机，有如下问题：

- 噪声：如果数据不是可分离的，那么权重可能会出现波动
- 过度训练：测试的准确性通常会上升，然后下降

而且，对于$z=w\cdot f(x)$，无论$z=0.1,z=100$都会产生$+1$

我们希望：

- $z$正数越大，$+1$概率趋近100%
- $z$和0相近，$+1$概率趋近50%
- $z$负数越小，$+1$概率趋近0

有**S型曲线 Sigmoid Function**满足该特性：
$$
\phi(z)=\frac{1}{1+e^{-z}}
$$
![Sigmoid Function](./data/Sigmoid Function.png)

**最大似然估计**为：
$$
\max_w ll(w)=\max_w\sum_i\log{P(y^{(i)}|x^{(i)};w)}
$$
其中
$$
\begin{align}
P(y^{(i)}&=+1|x^{(i)};w)=\frac{1}{1+e^{-w\cdot f(x^{(i)})}}\\
P(y^{(i)}&=-1|x^{(i)};w)=1-\frac{1}{1+e^{-w\cdot f(x^{(i)})}}
\end{align}
$$
这被成为**逻辑回归**

### 多级逻辑回归

由于要运用概率的形式，因此需要对多元感知机分数（激活activation）做归一化

假设原来分数值为$z_1,z_2,z_3$，则新的**软性激活**为$\frac{z_1}{z_1+z_2+z_3},\frac{z_2}{z_1+z_2+z_3},\frac{z_3}{z_1+z_2+z_3}$

**最大似然估计**为：
$$
\max_w ll(w)=\max_w\sum_i\log{P(y^{(i)}|x^{(i)};w)}
$$
其中
$$
P(y^{(i)}|x^{(i)};w)=\frac{e^{w_{y^{(i)}}\cdot f(x^{(i)})}}{\sum_ye^{w_{y^{(i)}}\cdot f(x^{(i)})}}
$$
**如何来解函数$\max_w ll(w)=\max_w\sum_i\log{P(y^{(i)}|x^{(i)};w)}$？**

考虑CSP中的爬山算法：从任意点开始，重复向最佳的相邻态移动，如果没有相邻态更佳，则推出。然而，如果优化实在连续域上的，将会有无穷多个相邻态。

![逻辑回归爬山法](./data/逻辑回归爬山法.png)

- 方法一：计算$g(w_0+h)$和$g(w_0-h)$，并向值更大的方向移动
- 方法二：计算导数$\frac{\partial g(w_0)}{\partial w}=\lim_{h\rightarrow0}\frac{g(w_0+h)-g(w_0-h)}{2h}$，根据导数的正负来判断方向

#### 梯度上升法 Gradient Ascent

想法：对每个坐标进行上坡方向的更新，坡度越大，该坐标的步幅就越大

考虑$g(w_1,w_2)$，对特征权重的更新为
$$
w_1\leftarrow w_1 + \alpha*\frac{\partial g}{\partial w_1}(w_1,w_2)\\
w_2\leftarrow w_2 + \alpha*\frac{\partial g}{\partial w_2}(w_1,w_2)\\
$$
更一般化的写法为
$$
w\leftarrow w+\alpha*\nabla_wg(w)
$$
其中**梯度**为$\nabla_wg(w)=\left[\begin{matrix}\frac{\partial g}{\partial w_1}(w)\\\frac{\partial g}{\partial w_2}(w)\end{matrix}\right]$

将该方法用于逻辑回归，可得步骤为

- 初始化$w$

- 对于每个样本：
  $$
  \begin{align}
  w&\leftarrow w+\alpha*\sum_i\nabla\log{P(y^{(i)}|x^{(i)};w)}\\
  &=w+\alpha*\sum_i\nabla\log{\frac{e^{w_{y^{(i)}}\cdot f(x^{(i)})}}{\sum_ye^{w_{y^{(i)}}\cdot f(x^{(i)})}}}\\
  &=w+\alpha*\sum_i(\nabla w_{y^{(i)}}\cdot f(x^{(i)})-\nabla\log{\sum_ye^{w_{y^{(i)}}\cdot f(x^{(i)})}})
  \end{align}
  $$

#### 随机梯度上升

![随机梯度上升](./data/随机梯度上升.png)

#### 小批量梯度上升

![批量梯度上升](./data/批量梯度上升.png)

# 十三、非监督学习

非监督学习的训练集不包含期望的输出

## 举例：群聚问题

想法：将相似的样本聚合至一起

“相似”的定义：

- 点集：可以用欧拉距离来区分
- 邮件：同一类型的邮件
- 搜索结果

## K-Means算法

K-Means算法是一种迭代的群居算法

- 随机选择K个点作为群聚中心点
- 迭代：
  - 将数据样本分给最近的中心点
  - 更新中心点为群聚的平均点
- 重复直到没有点的所属的群聚变化

### K-Means具体实现

定义到所有中心点的总距离为：
$$
\phi(\{x_i\},\{a_i\},\{c_k\})=\sum_idist(x_i,c_{a_i})
$$
其中

- $x_i$：点坐标
- $a_i$：分配的聚落
- $c_k$：$k$聚落的中心点

每一次迭代有两个阶段：

1. 更新分配：固定每一个聚落的中心，改变所有点的分配
2. 更新中心：固定所有点的分配，改变聚落的中心

注意：每一步的过程中都不能增加$\phi$

#### 步骤一：更新分配

对于每个点，将该点重新分配到最近的中心点
$$
a_i={\arg\min}_kdist(x_i,c_k)
$$
注意到 $\phi(\{x_i\},\{a_i\},\{c_k\})=\sum_idist(x_i,c_{a_i})$不增加

#### 步骤二：更新中心

将每个均值移动到其指定点的平均值
$$
c_k=\frac{1}{|\{i:a_i=k\}|}\sum_{i:a_i=k}x_i
$$
注意到有最小欧拉距离的到集合的点是它们的中心

### K-Means的不确定性

K-Means对初始选择的中心点要求很高，否则可能会出现局部优化

![KMeans不确定性](./data/KMeans不确定性.png)

## 期望最大化 EM

K-Means将数据分配到最近的中心，但有些集群可能比其他集群更宽，甚至集群间有重叠

需要一个允许重叠、不同大小、形状集群的概率模型

### 高斯混合模型 GMM

#### 高斯分布

$$
P(x|\mu,\sigma)=\frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}
$$

![高斯分布](./data/高斯分布.png)

#### 混合高斯分布

$$
p(x)=\sum^K_{k=1}\pi_k\mathcal{N}(x|\mu_k,\sum_k)
$$

其中

- $\pi_k$：混合系数
- $\mathcal{N}(x|\mu_k,\sum_k)$：高斯分布
- 归一化条件：$\forall k:\pi_k\geq0,\quad \sum^K_{k=1}\pi_k=1$

#### 高斯混合模型

- $P(Y)$：$k$个聚落分量的分布
- $P(X|Y)$：每个分量从一个均值$\mu_i$和协方差矩阵$\sum_i$的多元高斯分布中生成的数据

每一个数据点都从**生成过程**中采样：

1. 以$\pi_i$的概率选择聚落$i$
2. 从$N(x|\mu_i,\sum_i)$中生成数据点

![高斯混合模型](./data/高斯混合模型.png)

### 非监督学习中的高斯混合

我们可以观察数据点和它们的标签（由其高斯聚落分量生成），但需要确定高斯混合模型的参数

目标：最大化可能性
$$
\prod_jP(y_j=i,x_j)=\prod_j\pi_i\mathcal{N}(x_j|\mu,\sum_i)
$$
**封闭式解**：$m$个数据点，对于聚落分量$i$，假设有$n$个数据点，则
$$
\mu_i=\frac{1}{n}\sum^n_{j=1}x_j\qquad \sum_i=\frac{1}{N}\sum^n_{j=1}(x_j-\mu_i)(x_j-\mu_i)^T\qquad \pi_i=\frac{n}{m}
$$
但是对于聚落问题，标签$Y$是未知的，而最大化边缘可能性：
$$
\prod_jP(x_j)=\prod_j\sum_iP(y_i=i,x_j)=\prod_j\sum_i\pi_i\mathcal{N}(x_j|\mu,\sum_i)
$$
通过观察，可以发现两个事实：

- 已知混合高斯分布，类型很容易给出
- 已知类型，混合高斯分布也很容易找到

**期望最大值 EM：**

- 选择K个随机簇高斯模型
- E Step：计算每个实例具有每个可能标签的概率，将数据实例按比例分配给不同的模型
- M Step：根据分配的点（按比例）修改每个聚类模型
- 重复E Step和M Step直至收敛

**迭代：**在第$t$个迭代有参数估计
$$
\theta^{(t)}=\{\mu_1^{(t)},...,\mu_k^{(t)},\sum_1^{(t)},...,\sum_k^{(t)},\pi_1^{(t)},...,\pi_k^{(t)}\}
$$
**E-Step：**计算每个点的标签分布
$$
P(y_j=i|x_j,\theta^{(t)})\propto\pi_i^{(t)}\mathcal{N}(x_j|\mu_i^{(t)},\sum_i^{(t)})
$$
**M-Step：**给定标签分布，计算参数的MLE

![EM参数估计](./data/EM参数估计.png)

## EM 与 K-Means

假设所有高斯分布是球形的，具有相同的权重和协方差（唯一的参数是均值），且在E-Step的标签分布是点估计，则**EM可以退化为K-Means**

**EM**：可用于学习任何带有隐藏变量（缺失数据）的模型

# 十四、自然语言处理 NLP

**NLP**：自动分析、生成和获取人类自然语言

## 成分分析 Constituency Parsing

**成分分析树：**每一个非叶子节点代表一个词组

- S：语句
- NP：名词
- VP：动词

![成分分析树](./data/成分分析树.png)

## 语法 Grammars

**语法：**组成成分的集合和支配它们如何结合的规则

### 无文本语法 CFG

一个无文本语法有四个组成部分：

- $\sum$：终端（单词）集合
- $N$：非终端（词组）集合
- $S\in N$：起始符号
- $R$：生产规则集合，指定非终端如何产生终端或非终端的字符串

### 句子生成

语法可以用来生成一个字符串：

- 从一个只包含起始符号$S$的字符串开始
- 递归应用规则来重写字符串
- 直到字符串只包含终端

生成过程指定了字符串的语法结构（分析树）

### 句子分析

**解析：**输入一个字符串和一个语法，返回一个或多个分析树的过程

如果无法构造分析树，该字符串不属于该语言

**分析算法：**CYK，Earley等

### 概率语法

每条规则都与一个概率相关联：
$$
\alpha\rightarrow\beta:P(\alpha\rightarrow\beta|\alpha)
$$
**分析树的概率**：用于生成该分析树的所有规则概率的乘积

### 模糊性

如果一个句子有**多个可能的分析树**，那么该句子定义为**模糊**

## 语句分析算法

分析无文本语法CFG：将适当的分析树分配给输入字符串

### 暴力枚举

枚举所有与输入字符串一致的分析树

复杂度：具有$n$个叶子的二叉树的数量为卡特兰数$C_{n-1}$（指数级增长）

### CYK

CYK算法：**自下而上**的**动态编程**算法，适用于CNF，只产生**两类生产规则**，即
$$
A\rightarrow B\ C\qquad A\rightarrow w
$$
任何CFG都可以转换为CNF，产生的语法接受（或拒绝）的字符串集与原始语法相同

#### 动态编程

将问题分成许多子问题，子问题为分析位置$i$到位置$j$的子串

较小的子问题的解决方案在解决较大的子问题时被重新使用

![CYK动态编程](./data/CYK动态编程.png)

#### CYK表格

建立一个表格，使输入中从$i$到$j$的非终端$A$被放在表格的$[i-1,j]$单元格中

![CYK表格](./data/CYK表格.png)

一个跨越整个字符串的非终端将位于单元格$[0,n]$中，**从下往上**填表

#### CYK算法

- 基础情况：A在单元格$[i-1,i]$当且仅当存在规则使$A\rightarrow w_i$
- 递归情况：A在单元格$[i,j]$当且仅当存在存在规则$A\rightarrow B,C$，其中$B$在单元格$[i,k]$且$C$在单元格$[k,j]$

![CYK算法](./data/CYK算法.png)

我们可以得到表格，但由于要建立分析树，因此

- 在**构建过程中**，需要添加后向指针，使每个状态都知道来源

- 在**构建完表格后**，递归地从顶部（起始符号）向下检索成分

![CYK案例](./data/CYK案例.png)

#### CYK模糊情况处理

目标：找到最高概率的分析树

仍运行CYK，但

- 在表格的$[i-1,j]$单元格中，将每个非终端A与根植于A的最佳分析树的概率联系起来，该解析树覆盖$i$到$j$的子串

## 依赖关系分析

依赖关系分析是一颗**有向树**，其中

- 节点：句子中的单词
- ROOT：特殊的根节点
- 词与词之间的链接代表它们的依赖关系，且有**标签**

![依赖关系分析树](./data/依赖关系分析树.png)

优势：

- 在自由语序的语言中，依赖关系分析比成分分析更稳定
- 依赖性结构能捕捉到句法关系
- 解析速度比基于CFG的解析器更快

![依赖关系图](./data/依赖关系图.png)

需要在该图中找到箭头分数最高的序列，即John saw Mary
