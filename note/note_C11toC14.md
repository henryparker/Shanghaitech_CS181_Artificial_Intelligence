# 十一、强化学习 Reinforcement Learning

## 双老虎机案例

考虑面前有两个老虎机，分别为红色和蓝色。其中蓝色老虎机每次拉动拉杆有$P=1$的概率获得一美元，红色老虎机每次拉动拉杆有$P=0.75$的概率获得二美元，有$P=0.25$的概率获得零美元。求给定总拉动次数为$100$的情况下，获得最多美元的策略。

![双老虎机](./data/双老虎机.png)

解决：可以通过MDP的方法进行解决，只需要提前计算而不需要真正进行老虎机的拉动。最终结论为，玩红色老虎机效用为$150$美元，蓝色老虎机效用为$100$美元。

对于该案例，假设红色老虎机获得二美元和零美元的概率是未知的，而我们仅仅能观察到拉动红色老虎机的获利，再次考虑获得美元最多的策略：

- 这是一个学习问题，而非计划问题
- **强化学习**的概念由此引出，我们需要通过采样观察得到信息，并利用这些信息来进行判断，尽管这些信息可能不准确。

相较于MDP，强化学习的转移模型$T(s,a,s')$和奖励模型$R(s,a,s')$是未知的，需要环境问题本身给出观察结果

## 根据模型的学习

核心思想：根据采样估计转移模型和奖励模型的值$\hat{T}(s,a,s'),\hat{R}(s,a,s')$

![根据模型的学习](./data/根据模型的学习.png)

假设我们要求上海科技大学学生的期望年龄，即
$$
E[A]=\sum_aP(a)\cdot a=0.35\times 20 +...
$$
我们通过采样$[a_1,a_2,...,a_N]$，估算$\hat{P}(a)=\frac{num(a)}{N}$，进而求得$E[A]\approx\sum_a\hat{P}(a)\cdot a$

然而，对于**和模型无关的学习**，$E[A]\approx \frac{1}{N}\sum_ia_i$

## 和模型无关的学习

### 被动强化学习

核心思想：给定**固定策略$\pi(s)$**，按照该策略行动的同时，使用直接效用估计学习**状态的效用**

#### 直接效用估计

目标：在给定策略$\pi$的情况下计算每个状态的效用

步骤：

1. 根据$\pi$行动
2. 每到达一个状态，记录当前**折扣回报总和**（包括之前状态）
3. 平均这些样本

优点：易于理解，不需要$T$和$R$，最终能估计正确的均值

缺点：状态的效用并非相互独立的，每个状态都要分别学习，收敛速度慢

#### 策略评估

我们发现直接效用估计和策略评估十分相近，而策略评估考虑到了状态效用的关联性。策略评估有如下贝尔曼方程：
$$
\begin{align}
&V^\pi_{0}(s)=0\\
&V^\pi_{k+1}(s)\leftarrow\sum_{s'}T(s,\pi(s),s')[R(s,\pi(s),s')+\gamma V^\pi_k(s')]
\end{align}
$$
尽管我们不知道$T$和$R$的值，我们可以通过采样来估算
$$
\begin{align}
&sample_1=R(s,\pi(s),s^{'}_1)+\gamma V^\pi_k(s^{'}_1)\\
&sample_2=R(s,\pi(s),s^{'}_2)+\gamma V^\pi_k(s^{'}_2)\\
&...\\
&sample_n=R(s,\pi(s),s^{'}_n)+\gamma V^\pi_k(s^{'}_n)\\
\end{align}
$$

$$
V^\pi_{k+1}(s)\leftarrow \frac{1}{n}\sum_isample_i
$$

缺点：样本需要顺序采集

#### 时序差分学习

核心想法：在每次观察后立刻学习，更新$V(s)$

- $V(s)$的采样：$sample=R(s,\pi(s),s')+\gamma V^\pi(s')$
- $V(s)$的更新：$V^\pi(s)\leftarrow (1-\alpha)V^\pi(s)+(\alpha) sample$
- 同样的更新：$V^\pi(s)\leftarrow V^\pi(s)+\alpha(sample-V^\pi(s))$

其中蕴含着**指数平均移动**的思想：

- 运行中的插值更新：$\bar{x}_n=(1-\alpha)\cdot \bar{x}_{n-1}+\alpha \cdot x_n$
  $$
  \bar{x}_n=\frac{x_n+(1-\alpha)\cdot x_{n-1}+(1-\alpha)^2\cdot x_{n-2}+...}{1+(1-\alpha)+(1-\alpha)^2+...}
  $$

- 使最近的采样更加重要
- 降低的学习率$\alpha$能够使给出收敛的平均值

时序差分学习的限制性：

- 如果想将价值转变为新的策略，即$\pi(s)=argmax_aQ(s,a)=argmax_a\sum_{s'}T(s,a,s')[R(s,a,s')+\gamma V(s')]$，$T(s,a,s')$和$R(s,a,s')$是未知的

### 主动强化学习

目标：**学习最优的策略或效用**

#### Q学习 Q-Learning

- Q学习的价值迭代
  $$
  Q_{k+1}(s,a)\leftarrow \sum_{s'}T(s,a,s')[R(s,a,s')+\gamma max_{a'}Q_k(s',a')]
  $$

- 步骤：

  - 接收一个采样$(s,a,s',r)$
  - 考虑旧的估计：$Q(s,a)$
  - 考虑新的样本估计：$sample=R(s,a,s')+\gamma max_{a'}Q(s',a')$
  - 将新的估计并入运行平均值：$Q(s,a)\leftarrow (1-\alpha)Q(s,a)+(\alpha)[sample]$

特性：

- Q学习是一种与**策略无关的学习**，这意味着尽管选择不是最优的，Q学习也会收敛到最优策略

## 根据MDP和问题特性选择方法

![根据MDP和问题选择方法](./data/根据MDP和问题选择方法.png)

## 探索策略选择

### $\varepsilon$贪心

制定一个概率$P=\varepsilon$：

- 有$\varepsilon$的概率随机选择方向
- 有$1-\varepsilon$的概率选择当前最优策略

同时随着时间降低$\varepsilon$的值

### 探索方程

探索那些还未被充分探索的地方，最终将停止探索

**核心思想**：通过修改Q值来选择行动

效用方程定义为：
$$
f(u,n)=u+\frac{k}{n}
$$
其中$u$为Q值的估计，$k$为一个常数，$n$为该状态访问的次数

**Q值更新**：

- 通常情况下：$Q(s,a)\leftarrow_\alpha R(s,a,s')+\gamma max_{a'}Q(s',a')$
- 修改情况下：$Q(s,a)\leftarrow_\alpha R(s,a,s')+\gamma max_{a'}f(Q(s',a'),N(s',a'))$

### 近似Q学习

标准的Q学习需要维护记录所有状态的Q值表，而通常情况状态数量过多，难以遍历或记录

想法：通过经验学习少量的训练状态，将这些经验归纳到新的、相近的状态

#### 基于特征的表述

通过一数组的特征来描述一个状态，比如对于吃豆人，可能有以下特征：

- 距离最近鬼的距离
- 距离最近豆子的距离
- 鬼的数量
- 豆子的数量
- 吃豆人是否在一个通道内被鬼包夹？

通过以上的特征描述，我们可以用权重来得到任何状态的q函数：
$$
V(s)=w_1f_1(s)+w_2f_2(s)+...+w_nf_n(s)\\
Q(s,a)=w_1f_1(s,a)+w_2f_2(s,a)+...+w_nf_n(s,a)
$$
优势：所有的经验被总结为几个强有力的数字

劣势：各个状态可能有共同的特点，但实际上在价值上有很大的不同

步骤：

- 接收一个采样$(s,a,s',r)$
- 计算**差**：$difference=[r+\gamma max_{a'}Q(s',a')]-Q(s,a)$
- 近似特征值更新：$w_i\leftarrow w_i+\alpha[difference]f_i(s,a)$

Note：通常Q值更新，$Q(s,a)\leftarrow Q(s,a)+\alpha[difference]$

关于特征值更新的解释：调整有效特征的权重，如果发生错误，对引起该错误的特征降权重

##### 相关推广

- 多项式近似Q值：$Q(s,a)=w_{11}f_1(s,a)+w_{12}f_1(s,a)^2+w_{13}f_1(s,a)^3+...$

- 神经网络：$Q(s,a)=w_1f_1(s,a)+w_2f_2(s,a)+...+w_nf_n(s,a)$，其中参数也是经由训练得到
  $$
  w_m\leftarrow w_m+\alpha[r+\gamma max_a Q(s',a')-Q(s,a)]\frac{dQ}{dw_m}(s,a)
  $$

### 策略搜索

由于基于特征的表述预测性过于强烈，可能为了使特征最大，选择当前非奖励最大的行动

想法：学习使奖励最大化的政策，而不是预测奖励的值，从一个确定的解决方案开始（例如，近似Q学习），然后微调特征权重以找到更好的政策

最简单的策略搜索步骤：从一个初始的线性Q近似开始，改变每个特征的权重，测试该策略是否更好

# 十二、监督学习

监督学习是一个被定义的过程

- 目标：学习一个未知的目标函数$f$
- 输入：含有有**标签**$(x_j,y_j),y_j=f(x_j)$的训练集
- 输出：接近$f$的假设$h$

典型问题：

- 分类问题：学习具有离散的输出值$f$
- 回归问题：学习具有实数域的输出值的$f$
- 结构化预测：学习具有结构化输出的$f$

## 分类问题

### 垃圾邮件过滤器 Spam Filter

- 输入：一封电子邮件
- 输出：垃圾邮件（spam）或正常邮件（ham）

初始化：获取大量已分类的样本邮件，每封都已标记垃圾邮件或正常邮件，期望预测新的、未来的电子邮件的标签

垃圾邮件的可能特征：

- 文字：FREE!
- 文本模式：$, CAPS
- 非文本内容

### 数字识别 Digit Recognition

- 输入：图像，像素网格
- 输出：数字0~9

初始化：获得大量的示例图像，每张图像都标有一个数字，期望预测新的、未来的数字图像的标签

可能特征：

- 某一个像素是黑色的
- 形状特征：组成部分，曲率
